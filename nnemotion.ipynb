{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a32a97-ebf1-48cc-9a1a-578de3a8723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/x-akolli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/x-akolli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/x-akolli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/tmp/ipykernel_4191071/4156330684.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if(df.iloc[row][emo] == True):\n",
      "/tmp/ipykernel_4191071/4156330684.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if(df.iloc[row][emo] == False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6154, 250) (6154, 13)\n",
      "(684, 250) (684, 13)\n",
      "Epoch 1/10\n",
      "87/87 [==============================] - 11s 101ms/step - loss: 0.4385 - accuracy: 0.2111 - val_loss: 0.3904 - val_accuracy: 0.0844\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.3925 - accuracy: 0.2555 - val_loss: 0.3672 - val_accuracy: 0.2760\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 0.3333 - accuracy: 0.3852 - val_loss: 0.3237 - val_accuracy: 0.4205\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 0.2879 - accuracy: 0.4745 - val_loss: 0.3219 - val_accuracy: 0.4399\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 0.2585 - accuracy: 0.5560 - val_loss: 0.3099 - val_accuracy: 0.4821\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 9s 99ms/step - loss: 0.2313 - accuracy: 0.5807 - val_loss: 0.3144 - val_accuracy: 0.5081\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.2116 - accuracy: 0.6038 - val_loss: 0.3129 - val_accuracy: 0.4594\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 8s 97ms/step - loss: 0.1940 - accuracy: 0.6029 - val_loss: 0.3244 - val_accuracy: 0.5146\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "example = dataset['train'][0]\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\")) - set([\"not\"])\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "    text = BAD_SYMBOLS_RE.sub('', text) \n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "df['Tweet'] = df['Tweet'].apply(clean_text)\n",
    "df['Tweet'] = df['Tweet'].str.replace('\\d+', '')\n",
    "\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['Tweet'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Tweet'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "df.iloc[:, 2:] = df.iloc[:, 2:].astype(int)\n",
    "temp_column = []\n",
    "for row in range(len(df)):\n",
    "  temp_list = []\n",
    "  for emo in range(len(df.iloc[row])):\n",
    "    if(df.iloc[row][emo] == True):\n",
    "      temp_list.append(1)\n",
    "    if(df.iloc[row][emo] == False):\n",
    "       temp_list.append(0)\n",
    "  temp_list.insert(0, 0)\n",
    "  temp_list.insert(0, 0)\n",
    "  temp_column.append(temp_list)\n",
    "df['Sentiments'] = temp_column\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Tweet'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "temp_column = pd.DataFrame(temp_column)\n",
    "Y = temp_column.values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "y_train = np.asarray(Y_train).astype('float32').reshape((-1,1))\n",
    "y_test = np.asarray(Y_test).astype('float32').reshape((-1,1))\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import (Dense, LSTM, Embedding, SpatialDropout1D)\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "filepath = 'nnshanti.sav'\n",
    "pickle.dump(model, open(filepath, 'wb'))\n",
    "\n",
    "# new_complaint = ['I am really excited and love my girlfriend.']\n",
    "# seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "# padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# pred = model.predict(padded)\n",
    "# labels = ['bruh', 'bruh', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "# print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc667c-4d1a-4cbd-9c16-924a3c9c2a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7685d40-8358-44ed-af3d-72b61850066c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a36b2-9fd6-4f8a-b8fd-9f752f6aecb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "seminar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
